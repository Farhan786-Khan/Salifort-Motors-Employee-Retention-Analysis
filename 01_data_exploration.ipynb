{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udcca Salifort Motors Employee Retention Analysis\n",
        "## Phase 1: Data Exploration and Understanding (PACE - Plan & Analyze)\n",
        "\n",
        "**Project Overview:** This notebook focuses on understanding our dataset, exploring employee retention patterns, and setting the foundation for our predictive model.\n",
        "\n",
        "**Business Context:** Salifort Motors is experiencing employee turnover challenges. Our goal is to identify key factors driving attrition and build predictive capabilities to support HR decision-making.\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udccb Table of Contents\n",
        "1. [Project Setup & Data Loading](#setup)\n",
        "2. [Data Overview & Quality Assessment](#overview)\n",
        "3. [Exploratory Data Analysis](#eda)\n",
        "4. [Key Insights & Observations](#insights)\n",
        "5. [Next Steps](#next-steps)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udee0\ufe0f Project Setup & Data Loading {#setup}\n",
        "\n",
        "Let's start by importing necessary libraries and loading our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set visualization style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "print(\"\u2705 Libraries imported successfully!\")\n",
        "print(f\"\ud83d\udcca Pandas version: {pd.__version__}\")\n",
        "print(f\"\ud83d\udcc8 NumPy version: {np.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "# Note: Update the file path based on your data location\n",
        "try:\n",
        "    df = pd.read_csv('../data/raw/hr_dataset.csv')\n",
        "    print(f\"\u2705 Dataset loaded successfully!\")\n",
        "    print(f\"\ud83d\udccf Dataset shape: {df.shape}\")\n",
        "    print(f\"\ud83d\udc65 Number of employees: {df.shape[0]:,}\")\n",
        "    print(f\"\ud83d\udcca Number of features: {df.shape[1]}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"\u274c Dataset file not found. Please ensure the file is in the correct location.\")\n",
        "    print(\"Expected location: ../data/raw/hr_dataset.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcca Data Overview & Quality Assessment {#overview}\n",
        "\n",
        "Let's examine the structure and quality of our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display basic information about the dataset\n",
        "print(\"\ud83d\udccb DATASET INFORMATION\")\n",
        "print(\"=\" * 50)\n",
        "df.info()\n",
        "print(\"\\n\")\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\ud83d\udc40 FIRST 5 ROWS\")\n",
        "print(\"=\" * 50)\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"\ud83d\udd0d MISSING VALUES ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "missing_data = df.isnull().sum()\n",
        "missing_percent = (missing_data / len(df)) * 100\n",
        "\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing_data,\n",
        "    'Percentage': missing_percent\n",
        "}).sort_values('Missing Count', ascending=False)\n",
        "\n",
        "print(missing_df[missing_df['Missing Count'] > 0])\n",
        "\n",
        "if missing_df['Missing Count'].sum() == 0:\n",
        "    print(\"\u2705 No missing values found in the dataset!\")\n",
        "else:\n",
        "    print(f\"\u26a0\ufe0f  Total missing values: {missing_df['Missing Count'].sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical summary\n",
        "print(\"\ud83d\udcca STATISTICAL SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "display(df.describe())\n",
        "\n",
        "print(\"\\n\ud83d\udcc8 CATEGORICAL VARIABLES SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "for col in categorical_cols:\n",
        "    print(f\"\\n{col.upper()}:\")\n",
        "    print(df[col].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd0d Exploratory Data Analysis {#eda}\n",
        "\n",
        "Now let's dive deep into understanding patterns in our employee data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Target variable analysis\n",
        "print(\"\ud83c\udfaf TARGET VARIABLE ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Assuming 'left' is our target variable (adjust if different)\n",
        "target_col = 'left'  # Update this if your target column has a different name\n",
        "\n",
        "if target_col in df.columns:\n",
        "    retention_counts = df[target_col].value_counts()\n",
        "    retention_percent = df[target_col].value_counts(normalize=True) * 100\n",
        "    \n",
        "    print(f\"Employee Retention Status:\")\n",
        "    print(f\"Stayed: {retention_counts[0]:,} ({retention_percent[0]:.1f}%)\")\n",
        "    print(f\"Left: {retention_counts[1]:,} ({retention_percent[1]:.1f}%)\")\n",
        "    \n",
        "    # Visualize retention distribution\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # Count plot\n",
        "    sns.countplot(data=df, x=target_col, ax=ax1)\n",
        "    ax1.set_title('Employee Retention Distribution', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Employee Status (0=Stayed, 1=Left)')\n",
        "    ax1.set_ylabel('Count')\n",
        "    \n",
        "    # Pie chart\n",
        "    colors = ['#2E8B57', '#CD5C5C']\n",
        "    ax2.pie(retention_counts.values, labels=['Stayed', 'Left'], autopct='%1.1f%%', colors=colors, startangle=90)\n",
        "    ax2.set_title('Retention Rate Distribution', fontsize=14, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\u274c Target column 'left' not found. Please check column names.\")\n",
        "    print(f\"Available columns: {list(df.columns)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation analysis\n",
        "print(\"\ud83d\udd17 CORRELATION ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Select numeric columns for correlation\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "correlation_matrix = df[numeric_cols].corr()\n",
        "\n",
        "# Create correlation heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='RdYlBu_r', center=0, \n",
        "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
        "plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Show correlations with target variable\n",
        "if target_col in correlation_matrix.columns:\n",
        "    target_correlations = correlation_matrix[target_col].abs().sort_values(ascending=False)\n",
        "    print(f\"\\n\ud83c\udfaf Features most correlated with {target_col.upper()}:\")\n",
        "    print(target_correlations.drop(target_col).head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution analysis of key features\n",
        "print(\"\ud83d\udcca FEATURE DISTRIBUTION ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Key features to analyze (adjust based on your dataset)\n",
        "key_features = ['satisfaction_level', 'last_evaluation', 'number_project', \n",
        "                'average_montly_hours', 'time_spend_company']  # Update these names as needed\n",
        "\n",
        "# Filter features that exist in the dataset\n",
        "available_features = [col for col in key_features if col in df.columns]\n",
        "\n",
        "if available_features:\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    for i, feature in enumerate(available_features[:6]):\n",
        "        if i < len(axes):\n",
        "            sns.histplot(data=df, x=feature, hue=target_col if target_col in df.columns else None, \n",
        "                        kde=True, ax=axes[i], alpha=0.7)\n",
        "            axes[i].set_title(f'Distribution of {feature.replace(\"_\", \" \").title()}', \n",
        "                            fontweight='bold')\n",
        "            axes[i].set_xlabel(feature.replace(\"_\", \" \").title())\n",
        "    \n",
        "    # Remove empty subplots\n",
        "    for j in range(len(available_features), len(axes)):\n",
        "        fig.delaxes(axes[j])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f  Key features not found with expected names. Available columns:\")\n",
        "    print(list(df.columns))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udca1 Key Insights & Observations {#insights}\n",
        "\n",
        "Based on our exploratory analysis, here are the key findings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate automated insights based on the data\n",
        "print(\"\ud83d\udd0d AUTOMATED INSIGHTS GENERATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "insights = []\n",
        "\n",
        "# Dataset size insight\n",
        "insights.append(f\"\ud83d\udccf Dataset contains {df.shape[0]:,} employee records with {df.shape[1]} features\")\n",
        "\n",
        "# Missing data insight\n",
        "missing_count = df.isnull().sum().sum()\n",
        "if missing_count == 0:\n",
        "    insights.append(\"\u2705 Dataset is complete with no missing values\")\n",
        "else:\n",
        "    insights.append(f\"\u26a0\ufe0f  Dataset has {missing_count} missing values that need attention\")\n",
        "\n",
        "# Target variable insight\n",
        "if target_col in df.columns:\n",
        "    left_rate = df[target_col].mean() * 100\n",
        "    insights.append(f\"\ud83d\udcca Employee turnover rate: {left_rate:.1f}%\")\n",
        "    \n",
        "    if left_rate > 20:\n",
        "        insights.append(\"\ud83d\udea8 High turnover rate indicates significant retention challenges\")\n",
        "    elif left_rate > 10:\n",
        "        insights.append(\"\u26a0\ufe0f  Moderate turnover rate requires attention\")\n",
        "    else:\n",
        "        insights.append(\"\u2705 Low turnover rate indicates good retention\")\n",
        "\n",
        "# Feature correlation insights\n",
        "if target_col in df.columns and target_col in correlation_matrix.columns:\n",
        "    high_corr_features = correlation_matrix[target_col].abs().sort_values(ascending=False).drop(target_col).head(3)\n",
        "    insights.append(f\"\ud83d\udd17 Top predictive features: {', '.join(high_corr_features.index)}\")\n",
        "\n",
        "# Print insights\n",
        "for i, insight in enumerate(insights, 1):\n",
        "    print(f\"{i}. {insight}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"\ud83d\udcdd RECOMMENDATIONS FOR NEXT PHASE:\")\n",
        "print(\"=\"*50)\n",
        "recommendations = [\n",
        "    \"\ud83e\uddf9 Proceed to data cleaning and preprocessing\",\n",
        "    \"\ud83d\udcca Focus on highly correlated features for modeling\",\n",
        "    \"\ud83c\udfaf Consider feature engineering opportunities\",\n",
        "    \"\u2696\ufe0f  Address class imbalance if present\",\n",
        "    \"\ud83d\udd04 Validate findings with domain experts\"\n",
        "]\n",
        "\n",
        "for i, rec in enumerate(recommendations, 1):\n",
        "    print(f\"{i}. {rec}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\ude80 Next Steps {#next-steps}\n",
        "\n",
        "Based on our exploration, the next steps in our analysis will be:\n",
        "\n",
        "1. **Data Cleaning** (`02_data_cleaning.ipynb`)\n",
        "   - Handle any data quality issues\n",
        "   - Address outliers and anomalies\n",
        "   - Prepare data for modeling\n",
        "\n",
        "2. **Feature Engineering**\n",
        "   - Create new meaningful features\n",
        "   - Transform categorical variables\n",
        "   - Scale numerical features\n",
        "\n",
        "3. **Model Development** (`03_modeling.ipynb`)\n",
        "   - Build and compare multiple models\n",
        "   - Hyperparameter tuning\n",
        "   - Model validation\n",
        "\n",
        "4. **Results Analysis** (`04_results.ipynb`)\n",
        "   - Model interpretation\n",
        "   - Business insights\n",
        "   - Recommendations\n",
        "\n",
        "---\n",
        "\n",
        "**\ud83d\udcca Data Exploration Complete!** \n",
        "\n",
        "*Next notebook: `02_data_cleaning.ipynb`*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}