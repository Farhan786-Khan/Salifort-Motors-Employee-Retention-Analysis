{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83e\uddf9 Salifort Motors Employee Retention Analysis\n",
        "## Phase 2: Data Cleaning and Preprocessing (PACE - Analyze)\n",
        "\n",
        "**Project Overview:** This notebook focuses on cleaning and preprocessing our employee data to ensure high-quality input for our machine learning models.\n",
        "\n",
        "**Objectives:**\n",
        "- Handle missing values and outliers\n",
        "- Clean and standardize data formats\n",
        "- Create analysis-ready datasets\n",
        "- Prepare features for modeling\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udccb Table of Contents\n",
        "1. [Setup & Data Loading](#setup)\n",
        "2. [Data Quality Assessment](#quality)\n",
        "3. [Missing Values Treatment](#missing)\n",
        "4. [Outlier Detection & Treatment](#outliers)\n",
        "5. [Data Standardization](#standardization)\n",
        "6. [Feature Engineering](#features)\n",
        "7. [Final Dataset Export](#export)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udee0\ufe0f Setup & Data Loading {#setup}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set visualization style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"\u2705 Libraries imported successfully!\")\n",
        "print(\"\ud83d\udcca Ready for data cleaning and preprocessing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the raw dataset\n",
        "try:\n",
        "    df_raw = pd.read_csv('../data/raw/hr_dataset.csv')\n",
        "    print(f\"\u2705 Raw dataset loaded successfully!\")\n",
        "    print(f\"\ud83d\udccf Original shape: {df_raw.shape}\")\n",
        "    \n",
        "    # Create a working copy\n",
        "    df = df_raw.copy()\n",
        "    print(f\"\ud83d\udccb Working copy created for data cleaning\")\n",
        "    \n",
        "except FileNotFoundError:\n",
        "    print(\"\u274c Dataset file not found. Please ensure the file is in the correct location.\")\n",
        "    print(\"Expected location: ../data/raw/hr_dataset.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd0d Data Quality Assessment {#quality}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive data quality assessment\n",
        "print(\"\ud83d\udd0d COMPREHENSIVE DATA QUALITY ASSESSMENT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "quality_report = {\n",
        "    'Total Records': df.shape[0],\n",
        "    'Total Features': df.shape[1],\n",
        "    'Memory Usage (MB)': round(df.memory_usage(deep=True).sum() / 1024**2, 2)\n",
        "}\n",
        "\n",
        "print(\"\ud83d\udcca BASIC STATISTICS:\")\n",
        "for key, value in quality_report.items():\n",
        "    print(f\"  {key}: {value:,}\")\n",
        "\n",
        "print(\"\\n\ud83d\udccb COLUMN INFORMATION:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'Column Name':<25} {'Type':<15} {'Missing':<10} {'Unique':<10}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for col in df.columns:\n",
        "    missing_count = df[col].isnull().sum()\n",
        "    missing_pct = f\"{(missing_count/len(df)*100):.1f}%\"\n",
        "    unique_count = df[col].nunique()\n",
        "    dtype = str(df[col].dtype)\n",
        "    \n",
        "    print(f\"{col:<25} {dtype:<15} {missing_pct:<10} {unique_count:<10}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for duplicate records\n",
        "print(\"\ud83d\udd04 DUPLICATE RECORDS CHECK\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "duplicate_count = df.duplicated().sum()\n",
        "duplicate_pct = (duplicate_count / len(df)) * 100\n",
        "\n",
        "print(f\"Total duplicate records: {duplicate_count:,} ({duplicate_pct:.2f}%)\")\n",
        "\n",
        "if duplicate_count > 0:\n",
        "    print(\"\\n\ud83d\udd0d Sample duplicate records:\")\n",
        "    duplicates = df[df.duplicated(keep=False)].sort_values(list(df.columns))\n",
        "    display(duplicates.head(10))\n",
        "    \n",
        "    # Remove duplicates\n",
        "    df_before = df.shape[0]\n",
        "    df = df.drop_duplicates()\n",
        "    df_after = df.shape[0]\n",
        "    \n",
        "    print(f\"\\n\u2705 Duplicates removed: {df_before - df_after:,} records\")\n",
        "    print(f\"\ud83d\udccf New dataset shape: {df.shape}\")\n",
        "else:\n",
        "    print(\"\u2705 No duplicate records found!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83e\ude79 Missing Values Treatment {#missing}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed missing values analysis\n",
        "print(\"\ud83e\ude79 MISSING VALUES ANALYSIS & TREATMENT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "missing_data = df.isnull().sum()\n",
        "missing_percent = (missing_data / len(df)) * 100\n",
        "\n",
        "missing_df = pd.DataFrame({\n",
        "    'Column': missing_data.index,\n",
        "    'Missing_Count': missing_data.values,\n",
        "    'Missing_Percentage': missing_percent.values\n",
        "}).sort_values('Missing_Count', ascending=False)\n",
        "\n",
        "# Display missing values summary\n",
        "missing_summary = missing_df[missing_df['Missing_Count'] > 0]\n",
        "\n",
        "if len(missing_summary) > 0:\n",
        "    print(\"\ud83d\udcca Columns with missing values:\")\n",
        "    display(missing_summary)\n",
        "    \n",
        "    # Visualize missing data pattern\n",
        "    if len(missing_summary) > 0:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        sns.barplot(data=missing_summary, y='Column', x='Missing_Count')\n",
        "        plt.title('Missing Values Count by Column')\n",
        "        plt.xlabel('Number of Missing Values')\n",
        "        \n",
        "        plt.subplot(1, 2, 2)\n",
        "        sns.barplot(data=missing_summary, y='Column', x='Missing_Percentage')\n",
        "        plt.title('Missing Values Percentage by Column')\n",
        "        plt.xlabel('Percentage of Missing Values')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    # Handle missing values based on column type and percentage\n",
        "    print(\"\\n\ud83d\udd27 Missing values treatment strategy:\")\n",
        "    \n",
        "    for _, row in missing_summary.iterrows():\n",
        "        col = row['Column']\n",
        "        missing_pct = row['Missing_Percentage']\n",
        "        \n",
        "        if missing_pct > 50:\n",
        "            print(f\"  {col}: DROP COLUMN (>{missing_pct:.1f}% missing)\")\n",
        "            df = df.drop(columns=[col])\n",
        "        elif df[col].dtype in ['object']:\n",
        "            mode_val = df[col].mode()[0] if len(df[col].mode()) > 0 else 'Unknown'\n",
        "            print(f\"  {col}: FILL with mode '{mode_val}' ({missing_pct:.1f}% missing)\")\n",
        "            df[col].fillna(mode_val, inplace=True)\n",
        "        else:\n",
        "            median_val = df[col].median()\n",
        "            print(f\"  {col}: FILL with median {median_val} ({missing_pct:.1f}% missing)\")\n",
        "            df[col].fillna(median_val, inplace=True)\n",
        "    \n",
        "    print(f\"\\n\u2705 Missing values treatment completed!\")\n",
        "    print(f\"\ud83d\udccf Final dataset shape: {df.shape}\")\n",
        "    \n",
        "else:\n",
        "    print(\"\u2705 No missing values found in the dataset!\")\n",
        "\n",
        "# Verify no missing values remain\n",
        "remaining_missing = df.isnull().sum().sum()\n",
        "print(f\"\\n\ud83d\udd0d Remaining missing values: {remaining_missing}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udfaf Outlier Detection & Treatment {#outliers}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Outlier detection for numerical columns\n",
        "print(\"\ud83c\udfaf OUTLIER DETECTION & ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Remove target variable from outlier analysis if it exists\n",
        "target_col = 'left'  # Update if your target has a different name\n",
        "if target_col in numerical_cols:\n",
        "    numerical_cols.remove(target_col)\n",
        "\n",
        "print(f\"\ud83d\udcca Analyzing {len(numerical_cols)} numerical columns for outliers...\")\n",
        "\n",
        "outlier_summary = []\n",
        "\n",
        "# Function to detect outliers using IQR method\n",
        "def detect_outliers_iqr(column_data):\n",
        "    Q1 = column_data.quantile(0.25)\n",
        "    Q3 = column_data.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    \n",
        "    outliers = column_data[(column_data < lower_bound) | (column_data > upper_bound)]\n",
        "    return outliers, lower_bound, upper_bound\n",
        "\n",
        "# Detect outliers for each numerical column\n",
        "for col in numerical_cols:\n",
        "    outliers, lower_bound, upper_bound = detect_outliers_iqr(df[col])\n",
        "    outlier_count = len(outliers)\n",
        "    outlier_pct = (outlier_count / len(df)) * 100\n",
        "    \n",
        "    outlier_summary.append({\n",
        "        'Column': col,\n",
        "        'Outlier_Count': outlier_count,\n",
        "        'Outlier_Percentage': outlier_pct,\n",
        "        'Lower_Bound': round(lower_bound, 2),\n",
        "        'Upper_Bound': round(upper_bound, 2)\n",
        "    })\n",
        "\n",
        "# Display outlier summary\n",
        "outlier_df = pd.DataFrame(outlier_summary)\n",
        "print(\"\\n\ud83d\udcca Outlier Detection Summary:\")\n",
        "display(outlier_df[outlier_df['Outlier_Count'] > 0])\n",
        "\n",
        "# Visualize outliers using box plots\n",
        "if len(numerical_cols) > 0:\n",
        "    n_cols = min(3, len(numerical_cols))\n",
        "    n_rows = (len(numerical_cols) + n_cols - 1) // n_cols\n",
        "    \n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
        "    if n_rows == 1:\n",
        "        axes = [axes] if n_cols == 1 else axes\n",
        "    else:\n",
        "        axes = axes.flatten() if len(numerical_cols) > 1 else [axes]\n",
        "    \n",
        "    for i, col in enumerate(numerical_cols[:len(axes)]):\n",
        "        sns.boxplot(data=df, y=col, ax=axes[i])\n",
        "        axes[i].set_title(f'Outliers in {col.replace(\"_\", \" \").title()}')\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Hide extra subplots\n",
        "    for i in range(len(numerical_cols), len(axes)):\n",
        "        axes[i].set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Outlier treatment strategy\n",
        "print(\"\ud83d\udd27 OUTLIER TREATMENT STRATEGY\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create a copy for outlier treatment\n",
        "df_cleaned = df.copy()\n",
        "treatment_log = []\n",
        "\n",
        "for _, row in outlier_df.iterrows():\n",
        "    col = row['Column']\n",
        "    outlier_pct = row['Outlier_Percentage']\n",
        "    lower_bound = row['Lower_Bound']\n",
        "    upper_bound = row['Upper_Bound']\n",
        "    \n",
        "    if outlier_pct > 0:\n",
        "        if outlier_pct < 5:  # Conservative approach for < 5% outliers\n",
        "            # Cap outliers\n",
        "            original_outliers = ((df_cleaned[col] < lower_bound) | (df_cleaned[col] > upper_bound)).sum()\n",
        "            df_cleaned[col] = np.where(df_cleaned[col] < lower_bound, lower_bound, df_cleaned[col])\n",
        "            df_cleaned[col] = np.where(df_cleaned[col] > upper_bound, upper_bound, df_cleaned[col])\n",
        "            \n",
        "            treatment_log.append(f\"  {col}: CAPPED {original_outliers} outliers ({outlier_pct:.1f}%)\")\n",
        "            \n",
        "        elif outlier_pct >= 5 and outlier_pct < 15:  # Moderate outliers\n",
        "            # Use log transformation for right-skewed data\n",
        "            if df_cleaned[col].min() > 0:  # Can apply log transformation\n",
        "                df_cleaned[f'{col}_log'] = np.log1p(df_cleaned[col])\n",
        "                treatment_log.append(f\"  {col}: LOG TRANSFORMED (created {col}_log) ({outlier_pct:.1f}%)\")\n",
        "            else:\n",
        "                # Cap outliers if log transformation not possible\n",
        "                original_outliers = ((df_cleaned[col] < lower_bound) | (df_cleaned[col] > upper_bound)).sum()\n",
        "                df_cleaned[col] = np.where(df_cleaned[col] < lower_bound, lower_bound, df_cleaned[col])\n",
        "                df_cleaned[col] = np.where(df_cleaned[col] > upper_bound, upper_bound, df_cleaned[col])\n",
        "                treatment_log.append(f\"  {col}: CAPPED {original_outliers} outliers ({outlier_pct:.1f}%)\")\n",
        "        else:\n",
        "            # High percentage of outliers - investigate further\n",
        "            treatment_log.append(f\"  {col}: INVESTIGATE FURTHER - High outlier rate ({outlier_pct:.1f}%)\")\n",
        "\n",
        "if treatment_log:\n",
        "    print(\"\\n\ud83d\udccb Treatment applied:\")\n",
        "    for log in treatment_log:\n",
        "        print(log)\n",
        "else:\n",
        "    print(\"\u2705 No significant outliers requiring treatment!\")\n",
        "\n",
        "print(f\"\\n\ud83d\udccf Dataset shape after outlier treatment: {df_cleaned.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcca Data Standardization {#standardization}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data standardization and formatting\n",
        "print(\"\ud83d\udcca DATA STANDARDIZATION & FORMATTING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Work with cleaned dataset\n",
        "df_final = df_cleaned.copy()\n",
        "\n",
        "# Standardize column names (lowercase, replace spaces with underscores)\n",
        "original_columns = df_final.columns.tolist()\n",
        "df_final.columns = [col.lower().replace(' ', '_').replace('-', '_') for col in df_final.columns]\n",
        "new_columns = df_final.columns.tolist()\n",
        "\n",
        "print(\"\ud83c\udff7\ufe0f  Column name standardization:\")\n",
        "for old, new in zip(original_columns, new_columns):\n",
        "    if old != new:\n",
        "        print(f\"  '{old}' \u2192 '{new}'\")\n",
        "\n",
        "# Handle categorical variables\n",
        "categorical_cols = df_final.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "if categorical_cols:\n",
        "    print(f\"\\n\ud83d\udccb Processing {len(categorical_cols)} categorical columns:\")\n",
        "    \n",
        "    for col in categorical_cols:\n",
        "        # Standardize text values\n",
        "        df_final[col] = df_final[col].astype(str).str.strip().str.lower()\n",
        "        \n",
        "        # Show unique values\n",
        "        unique_vals = df_final[col].value_counts().head(10)\n",
        "        print(f\"\\n  {col}:\")\n",
        "        print(f\"    Unique values: {df_final[col].nunique()}\")\n",
        "        print(f\"    Top values: {list(unique_vals.head(5).index)}\")\n",
        "\n",
        "# Validate numerical columns\n",
        "numerical_cols_final = df_final.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(f\"\\n\ud83d\udd22 Numerical columns validated: {len(numerical_cols_final)}\")\n",
        "for col in numerical_cols_final:\n",
        "    print(f\"  {col}: {df_final[col].dtype} (Range: {df_final[col].min():.2f} - {df_final[col].max():.2f})\")\n",
        "\n",
        "print(f\"\\n\u2705 Data standardization completed!\")\n",
        "print(f\"\ud83d\udccf Final dataset shape: {df_final.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd27 Feature Engineering {#features}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature engineering for employee retention analysis\n",
        "print(\"\ud83d\udd27 FEATURE ENGINEERING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create engineered features based on common HR metrics\n",
        "features_created = []\n",
        "\n",
        "# 1. Work-life balance indicators (assuming columns exist)\n",
        "if 'average_montly_hours' in df_final.columns:\n",
        "    # Work intensity categories\n",
        "    df_final['work_intensity'] = pd.cut(df_final['average_montly_hours'], \n",
        "                                       bins=[0, 160, 200, 250, float('inf')], \n",
        "                                       labels=['normal', 'moderate', 'high', 'extreme'])\n",
        "    \n",
        "    # Overtime indicator\n",
        "    df_final['overtime'] = (df_final['average_montly_hours'] > 200).astype(int)\n",
        "    \n",
        "    features_created.extend(['work_intensity', 'overtime'])\n",
        "\n",
        "# 2. Performance indicators\n",
        "if 'last_evaluation' in df_final.columns and 'satisfaction_level' in df_final.columns:\n",
        "    # Performance-satisfaction matrix\n",
        "    df_final['perf_sat_ratio'] = df_final['last_evaluation'] / (df_final['satisfaction_level'] + 0.01)\n",
        "    \n",
        "    # High performer flag\n",
        "    df_final['high_performer'] = ((df_final['last_evaluation'] > 0.7) & \n",
        "                                 (df_final['satisfaction_level'] > 0.6)).astype(int)\n",
        "    \n",
        "    features_created.extend(['perf_sat_ratio', 'high_performer'])\n",
        "\n",
        "# 3. Project workload analysis\n",
        "if 'number_project' in df_final.columns:\n",
        "    # Project load categories\n",
        "    df_final['project_load'] = pd.cut(df_final['number_project'],\n",
        "                                     bins=[0, 2, 4, 6, float('inf')],\n",
        "                                     labels=['low', 'normal', 'high', 'extreme'])\n",
        "    \n",
        "    features_created.append('project_load')\n",
        "\n",
        "# 4. Tenure analysis\n",
        "if 'time_spend_company' in df_final.columns:\n",
        "    # Tenure categories\n",
        "    df_final['tenure_category'] = pd.cut(df_final['time_spend_company'],\n",
        "                                        bins=[0, 2, 5, 8, float('inf')],\n",
        "                                        labels=['new', 'experienced', 'veteran', 'senior'])\n",
        "    \n",
        "    features_created.append('tenure_category')\n",
        "\n",
        "# 5. Risk scores (combine multiple factors)\n",
        "risk_factors = []\n",
        "if 'satisfaction_level' in df_final.columns:\n",
        "    risk_factors.append('(1 - satisfaction_level)')\n",
        "if 'overtime' in df_final.columns:\n",
        "    risk_factors.append('overtime')\n",
        "if 'last_evaluation' in df_final.columns:\n",
        "    risk_factors.append('(1 - last_evaluation)')\n",
        "\n",
        "if risk_factors:\n",
        "    risk_formula = ' + '.join(risk_factors)\n",
        "    df_final['attrition_risk_score'] = eval(f\"df_final.eval('{risk_formula}')\")\n",
        "    features_created.append('attrition_risk_score')\n",
        "\n",
        "# Summary of engineered features\n",
        "if features_created:\n",
        "    print(f\"\u2705 Created {len(features_created)} new features:\")\n",
        "    for feature in features_created:\n",
        "        print(f\"  \u2022 {feature}\")\n",
        "        if df_final[feature].dtype == 'object' or hasattr(df_final[feature], 'cat'):\n",
        "            print(f\"    Values: {df_final[feature].value_counts().head(3).to_dict()}\")\n",
        "        else:\n",
        "            print(f\"    Range: {df_final[feature].min():.3f} - {df_final[feature].max():.3f}\")\n",
        "        print()\n",
        "else:\n",
        "    print(\"\u2139\ufe0f  No engineered features created (column names may need adjustment)\")\n",
        "\n",
        "print(f\"\ud83d\udccf Dataset shape after feature engineering: {df_final.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcbe Final Dataset Export {#export}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final data quality check and export\n",
        "print(\"\ud83d\udcbe FINAL DATASET PREPARATION & EXPORT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Final quality checks\n",
        "print(\"\ud83d\udd0d Final Quality Checks:\")\n",
        "print(f\"  \u2713 Dataset shape: {df_final.shape}\")\n",
        "print(f\"  \u2713 Missing values: {df_final.isnull().sum().sum()}\")\n",
        "print(f\"  \u2713 Duplicate records: {df_final.duplicated().sum()}\")\n",
        "print(f\"  \u2713 Memory usage: {df_final.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# Data types summary\n",
        "print(\"\\n\ud83d\udcca Data Types Summary:\")\n",
        "dtype_summary = df_final.dtypes.value_counts()\n",
        "for dtype, count in dtype_summary.items():\n",
        "    print(f\"  {dtype}: {count} columns\")\n",
        "\n",
        "# Save cleaned dataset\n",
        "try:\n",
        "    # Create processed data directory if it doesn't exist\n",
        "    import os\n",
        "    os.makedirs('../data/processed', exist_ok=True)\n",
        "    \n",
        "    # Save the cleaned dataset\n",
        "    output_path = '../data/processed/hr_dataset_cleaned.csv'\n",
        "    df_final.to_csv(output_path, index=False)\n",
        "    \n",
        "    print(f\"\\n\u2705 Cleaned dataset saved to: {output_path}\")\n",
        "    print(f\"\ud83d\udcca Records saved: {df_final.shape[0]:,}\")\n",
        "    print(f\"\ud83d\udccb Features saved: {df_final.shape[1]}\")\n",
        "    \n",
        "    # Save data dictionary\n",
        "    data_dict = pd.DataFrame({\n",
        "        'Column': df_final.columns,\n",
        "        'Data_Type': [str(dtype) for dtype in df_final.dtypes],\n",
        "        'Non_Null_Count': [df_final[col].count() for col in df_final.columns],\n",
        "        'Unique_Values': [df_final[col].nunique() for col in df_final.columns]\n",
        "    })\n",
        "    \n",
        "    dict_path = '../data/processed/data_dictionary.csv'\n",
        "    data_dict.to_csv(dict_path, index=False)\n",
        "    print(f\"\ud83d\udcd6 Data dictionary saved to: {dict_path}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error saving files: {str(e)}\")\n",
        "\n",
        "# Create summary statistics\n",
        "print(\"\\n\ud83d\udcc8 CLEANING SUMMARY STATISTICS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "summary_stats = {\n",
        "    'Original Records': df_raw.shape[0] if 'df_raw' in locals() else 'N/A',\n",
        "    'Final Records': df_final.shape[0],\n",
        "    'Original Features': df_raw.shape[1] if 'df_raw' in locals() else 'N/A',\n",
        "    'Final Features': df_final.shape[1],\n",
        "    'Records Removed': df_raw.shape[0] - df_final.shape[0] if 'df_raw' in locals() else 'N/A',\n",
        "    'Features Added': len(features_created) if features_created else 0,\n",
        "    'Data Quality Score': f\"{((1 - (df_final.isnull().sum().sum() / df_final.size)) * 100):.1f}%\"\n",
        "}\n",
        "\n",
        "for metric, value in summary_stats.items():\n",
        "    print(f\"  {metric}: {value}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"\u2705 DATA CLEANING & PREPROCESSING COMPLETE!\")\n",
        "print(\"\ud83d\udccb Ready for modeling phase\")\n",
        "print(\"\ud83d\ude80 Next step: 03_modeling.ipynb\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udccb Cleaning Summary\n",
        "\n",
        "### \u2705 Completed Tasks:\n",
        "1. **Data Quality Assessment** - Comprehensive analysis of data structure and quality\n",
        "2. **Missing Values Treatment** - Handled missing data using appropriate imputation strategies\n",
        "3. **Duplicate Removal** - Identified and removed duplicate records\n",
        "4. **Outlier Treatment** - Applied appropriate outlier detection and treatment methods\n",
        "5. **Data Standardization** - Standardized column names and data formats\n",
        "6. **Feature Engineering** - Created meaningful derived features for analysis\n",
        "7. **Data Export** - Saved cleaned dataset for modeling phase\n",
        "\n",
        "### \ud83d\udcca Data Quality Improvements:\n",
        "- Removed inconsistencies and standardized formats\n",
        "- Applied appropriate data types to all columns\n",
        "- Created analysis-ready features\n",
        "- Ensured data integrity for machine learning models\n",
        "\n",
        "### \ud83d\ude80 Next Steps:\n",
        "The cleaned dataset is now ready for the modeling phase where we will:\n",
        "1. Build predictive models for employee retention\n",
        "2. Compare different algorithms and approaches\n",
        "3. Optimize model performance through hyperparameter tuning\n",
        "4. Validate model results and interpretability\n",
        "\n",
        "---\n",
        "\n",
        "**\ud83d\udcca Data Cleaning Complete!**\n",
        "\n",
        "*Next notebook: `03_modeling.ipynb`*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}