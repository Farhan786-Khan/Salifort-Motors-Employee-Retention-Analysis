{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83e\udd16 Salifort Motors Employee Retention Analysis\n",
        "## Phase 3: Model Development and Training (PACE - Construct)\n",
        "\n",
        "**Project Overview:** This notebook focuses on building and training machine learning models to predict employee retention at Salifort Motors.\n",
        "\n",
        "**Objectives:**\n",
        "- Build multiple classification models (Logistic Regression, Random Forest, etc.)\n",
        "- Compare model performance using appropriate metrics\n",
        "- Optimize best performing model through hyperparameter tuning\n",
        "- Evaluate feature importance and model interpretability\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udccb Table of Contents\n",
        "1. [Setup & Data Loading](#setup)\n",
        "2. [Data Preparation for Modeling](#preparation)\n",
        "3. [Baseline Models](#baseline)\n",
        "4. [Model Comparison](#comparison)\n",
        "5. [Hyperparameter Tuning](#tuning)\n",
        "6. [Feature Importance Analysis](#importance)\n",
        "7. [Model Evaluation](#evaluation)\n",
        "8. [Model Persistence](#persistence)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udee0\ufe0f Setup & Data Loading {#setup}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Set visualization style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"\u2705 Libraries imported successfully!\")\n",
        "print(\"\ud83e\udd16 Ready for machine learning model development\")\n",
        "print(f\"\ud83c\udfb2 Random seed set to: 42\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the cleaned dataset\n",
        "try:\n",
        "    df = pd.read_csv('../data/processed/hr_dataset_cleaned.csv')\n",
        "    print(f\"\u2705 Cleaned dataset loaded successfully!\")\n",
        "    print(f\"\ud83d\udccf Dataset shape: {df.shape}\")\n",
        "    print(f\"\ud83d\udc65 Number of employees: {df.shape[0]:,}\")\n",
        "    print(f\"\ud83d\udcca Number of features: {df.shape[1]}\")\n",
        "    \n",
        "    # Display basic info\n",
        "    print(f\"\\n\ud83d\udccb Dataset columns: {list(df.columns)}\")\n",
        "    \n",
        "except FileNotFoundError:\n",
        "    print(\"\u274c Cleaned dataset not found. Please run 02_data_cleaning.ipynb first.\")\n",
        "    print(\"Expected location: ../data/processed/hr_dataset_cleaned.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcca Data Preparation for Modeling {#preparation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for machine learning\n",
        "print(\"\ud83d\udcca DATA PREPARATION FOR MACHINE LEARNING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Identify target variable (adjust column name if different)\n",
        "target_col = 'left'  # Update this if your target column has a different name\n",
        "\n",
        "# Check if target column exists\n",
        "if target_col not in df.columns:\n",
        "    # Try common variations\n",
        "    possible_targets = ['left', 'attrition', 'churn', 'turnover', 'quit']\n",
        "    for col in possible_targets:\n",
        "        if col in df.columns:\n",
        "            target_col = col\n",
        "            break\n",
        "    \n",
        "    if target_col not in df.columns:\n",
        "        print(f\"\u274c Target column not found. Available columns: {list(df.columns)}\")\n",
        "        print(\"Please update the target_col variable with the correct column name.\")\n",
        "\n",
        "print(f\"\ud83c\udfaf Target variable: {target_col}\")\n",
        "\n",
        "# Separate features and target\n",
        "if target_col in df.columns:\n",
        "    X = df.drop(columns=[target_col])\n",
        "    y = df[target_col]\n",
        "    \n",
        "    print(f\"\ud83d\udcca Features shape: {X.shape}\")\n",
        "    print(f\"\ud83c\udfaf Target shape: {y.shape}\")\n",
        "    \n",
        "    # Check target distribution\n",
        "    target_dist = y.value_counts(normalize=True)\n",
        "    print(f\"\\n\ud83d\udcc8 Target distribution:\")\n",
        "    for value, pct in target_dist.items():\n",
        "        label = 'Left' if value == 1 else 'Stayed'\n",
        "        print(f\"  {label}: {pct:.1%}\")\n",
        "    \n",
        "    # Check for class imbalance\n",
        "    if target_dist.min() < 0.3:\n",
        "        print(\"\u26a0\ufe0f  Class imbalance detected - consider using stratified sampling and appropriate metrics\")\n",
        "    else:\n",
        "        print(\"\u2705 Classes are relatively balanced\")\n",
        "    \n",
        "else:\n",
        "    print(\"\u274c Cannot proceed without target variable\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features for modeling\n",
        "print(\"\ud83d\udd27 FEATURE PREPARATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Identify column types\n",
        "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "print(f\"\ud83d\udd22 Numeric features ({len(numeric_features)}): {numeric_features}\")\n",
        "print(f\"\ud83d\udcdd Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
        "\n",
        "# Create preprocessing pipelines\n",
        "numeric_transformer = StandardScaler()\n",
        "\n",
        "# Handle categorical features if they exist\n",
        "if categorical_features:\n",
        "    categorical_transformer = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
        "    \n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ]\n",
        "    )\n",
        "else:\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "print(\"\u2705 Preprocessing pipeline created\")\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\n\ud83d\udcca Data split completed:\")\n",
        "print(f\"  Training set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X):.1%})\")\n",
        "print(f\"  Test set: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X):.1%})\")\n",
        "\n",
        "# Check stratification worked\n",
        "print(f\"\\n\ud83c\udfaf Target distribution after split:\")\n",
        "train_dist = y_train.value_counts(normalize=True).sort_index()\n",
        "test_dist = y_test.value_counts(normalize=True).sort_index()\n",
        "print(f\"  Training - Stayed: {train_dist[0]:.1%}, Left: {train_dist[1]:.1%}\")\n",
        "print(f\"  Test - Stayed: {test_dist[0]:.1%}, Left: {test_dist[1]:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\ude80 Baseline Models {#baseline}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build baseline models\n",
        "print(\"\ud83d\ude80 BUILDING BASELINE MODELS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Define models to compare\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100),\n",
        "    'Support Vector Machine': SVC(random_state=42, probability=True)\n",
        "}\n",
        "\n",
        "print(f\"\ud83e\udd16 Training {len(models)} baseline models...\")\n",
        "\n",
        "# Store results\n",
        "baseline_results = {}\n",
        "trained_models = {}\n",
        "\n",
        "# Train each model\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n\ud83d\udd04 Training {name}...\")\n",
        "    \n",
        "    # Create pipeline with preprocessing\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "    \n",
        "    # Train the model\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    y_pred_proba = pipeline.predict_proba(X_test)[:, 1] if hasattr(pipeline, \"predict_proba\") else None\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None\n",
        "    \n",
        "    # Store results\n",
        "    baseline_results[name] = {\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-Score': f1,\n",
        "        'ROC-AUC': auc\n",
        "    }\n",
        "    \n",
        "    trained_models[name] = pipeline\n",
        "    \n",
        "    print(f\"  \u2705 {name} - F1: {f1:.3f}, Accuracy: {accuracy:.3f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"\u2705 All baseline models trained successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcc8 Model Comparison {#comparison}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare model performances\n",
        "print(\"\ud83d\udcc8 MODEL PERFORMANCE COMPARISON\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame(baseline_results).T\n",
        "results_df = results_df.round(4)\n",
        "\n",
        "# Display results table\n",
        "print(\"\ud83c\udfc6 Model Performance Summary:\")\n",
        "display(results_df.sort_values('F1-Score', ascending=False))\n",
        "\n",
        "# Identify best model\n",
        "best_model_name = results_df['F1-Score'].idxmax()\n",
        "best_f1_score = results_df.loc[best_model_name, 'F1-Score']\n",
        "print(f\"\\n\ud83e\udd47 Best performing model: {best_model_name} (F1-Score: {best_f1_score:.4f})\")\n",
        "\n",
        "# Visualize model comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Plot 1: F1-Score comparison\n",
        "results_df.sort_values('F1-Score')['F1-Score'].plot(kind='barh', ax=axes[0,0], color='skyblue')\n",
        "axes[0,0].set_title('F1-Score Comparison', fontweight='bold')\n",
        "axes[0,0].set_xlabel('F1-Score')\n",
        "\n",
        "# Plot 2: Accuracy comparison\n",
        "results_df.sort_values('Accuracy')['Accuracy'].plot(kind='barh', ax=axes[0,1], color='lightgreen')\n",
        "axes[0,1].set_title('Accuracy Comparison', fontweight='bold')\n",
        "axes[0,1].set_xlabel('Accuracy')\n",
        "\n",
        "# Plot 3: Precision vs Recall\n",
        "axes[1,0].scatter(results_df['Recall'], results_df['Precision'], s=100, alpha=0.7)\n",
        "for i, model in enumerate(results_df.index):\n",
        "    axes[1,0].annotate(model, (results_df.iloc[i]['Recall'], results_df.iloc[i]['Precision']), \n",
        "                      xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
        "axes[1,0].set_xlabel('Recall')\n",
        "axes[1,0].set_ylabel('Precision')\n",
        "axes[1,0].set_title('Precision vs Recall', fontweight='bold')\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: ROC-AUC comparison (if available)\n",
        "roc_auc_data = results_df.dropna(subset=['ROC-AUC'])\n",
        "if not roc_auc_data.empty:\n",
        "    roc_auc_data.sort_values('ROC-AUC')['ROC-AUC'].plot(kind='barh', ax=axes[1,1], color='coral')\n",
        "    axes[1,1].set_title('ROC-AUC Comparison', fontweight='bold')\n",
        "    axes[1,1].set_xlabel('ROC-AUC Score')\n",
        "else:\n",
        "    axes[1,1].text(0.5, 0.5, 'ROC-AUC not available\\nfor all models', \n",
        "                  ha='center', va='center', transform=axes[1,1].transAxes)\n",
        "    axes[1,1].set_title('ROC-AUC Comparison', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save comparison results\n",
        "results_df.to_csv('../results/model_comparison.csv')\n",
        "print(f\"\\n\ud83d\udcbe Results saved to: ../results/model_comparison.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u2699\ufe0f Hyperparameter Tuning {#tuning}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter tuning for the best model\n",
        "print(\"\u2699\ufe0f HYPERPARAMETER TUNING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(f\"\ud83c\udfaf Tuning hyperparameters for: {best_model_name}\")\n",
        "\n",
        "# Define hyperparameter grids for different models\n",
        "param_grids = {\n",
        "    'Random Forest': {\n",
        "        'classifier__n_estimators': [100, 200, 300],\n",
        "        'classifier__max_depth': [10, 15, 20, None],\n",
        "        'classifier__min_samples_split': [2, 5, 10],\n",
        "        'classifier__min_samples_leaf': [1, 2, 4]\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'classifier__n_estimators': [100, 200],\n",
        "        'classifier__learning_rate': [0.05, 0.1, 0.15],\n",
        "        'classifier__max_depth': [3, 5, 7],\n",
        "        'classifier__subsample': [0.8, 0.9, 1.0]\n",
        "    },\n",
        "    'Logistic Regression': {\n",
        "        'classifier__C': [0.1, 1.0, 10.0, 100.0],\n",
        "        'classifier__penalty': ['l1', 'l2'],\n",
        "        'classifier__solver': ['liblinear', 'saga']\n",
        "    },\n",
        "    'Decision Tree': {\n",
        "        'classifier__max_depth': [5, 10, 15, 20, None],\n",
        "        'classifier__min_samples_split': [2, 5, 10, 20],\n",
        "        'classifier__min_samples_leaf': [1, 2, 5, 10],\n",
        "        'classifier__criterion': ['gini', 'entropy']\n",
        "    }\n",
        "}\n",
        "\n",
        "# Get the appropriate parameter grid\n",
        "if best_model_name in param_grids:\n",
        "    param_grid = param_grids[best_model_name]\n",
        "    \n",
        "    print(f\"\ud83d\udcca Parameter grid: {len(param_grid)} parameters to tune\")\n",
        "    for param, values in param_grid.items():\n",
        "        print(f\"  {param}: {values}\")\n",
        "    \n",
        "    # Set up cross-validation\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    \n",
        "    # Perform grid search\n",
        "    print(f\"\\n\ud83d\udd04 Performing grid search with 5-fold cross-validation...\")\n",
        "    print(f\"\ud83d\udcad This may take a few minutes...\")\n",
        "    \n",
        "    best_pipeline = trained_models[best_model_name]\n",
        "    grid_search = GridSearchCV(\n",
        "        best_pipeline, \n",
        "        param_grid, \n",
        "        cv=cv, \n",
        "        scoring='f1',  # Optimize for F1-score\n",
        "        n_jobs=-1,     # Use all available cores\n",
        "        verbose=1      # Show progress\n",
        "    )\n",
        "    \n",
        "    # Fit grid search\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    \n",
        "    # Get best model\n",
        "    best_tuned_model = grid_search.best_estimator_\n",
        "    \n",
        "    print(f\"\\n\u2705 Hyperparameter tuning completed!\")\n",
        "    print(f\"\ud83c\udfc6 Best CV F1-Score: {grid_search.best_score_:.4f}\")\n",
        "    print(f\"\u2699\ufe0f  Best parameters:\")\n",
        "    for param, value in grid_search.best_params_.items():\n",
        "        print(f\"  {param}: {value}\")\n",
        "    \n",
        "    # Evaluate tuned model on test set\n",
        "    y_pred_tuned = best_tuned_model.predict(X_test)\n",
        "    y_pred_proba_tuned = best_tuned_model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Calculate improved metrics\n",
        "    tuned_metrics = {\n",
        "        'Accuracy': accuracy_score(y_test, y_pred_tuned),\n",
        "        'Precision': precision_score(y_test, y_pred_tuned),\n",
        "        'Recall': recall_score(y_test, y_pred_tuned),\n",
        "        'F1-Score': f1_score(y_test, y_pred_tuned),\n",
        "        'ROC-AUC': roc_auc_score(y_test, y_pred_proba_tuned)\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n\ud83d\udcca Tuned model performance on test set:\")\n",
        "    for metric, value in tuned_metrics.items():\n",
        "        original_value = baseline_results[best_model_name][metric]\n",
        "        improvement = value - original_value if original_value is not None else None\n",
        "        if improvement is not None:\n",
        "            print(f\"  {metric}: {value:.4f} (+{improvement:+.4f})\")\n",
        "        else:\n",
        "            print(f\"  {metric}: {value:.4f}\")\n",
        "    \n",
        "    # Store the best model\n",
        "    final_model = best_tuned_model\n",
        "    final_predictions = y_pred_tuned\n",
        "    final_probabilities = y_pred_proba_tuned\n",
        "    \n",
        "else:\n",
        "    print(f\"\u26a0\ufe0f  No hyperparameter grid defined for {best_model_name}\")\n",
        "    print(f\"\ud83d\udcdd Using baseline model as final model\")\n",
        "    final_model = trained_models[best_model_name]\n",
        "    final_predictions = final_model.predict(X_test)\n",
        "    final_probabilities = final_model.predict_proba(X_test)[:, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd0d Feature Importance Analysis {#importance}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance analysis\n",
        "print(\"\ud83d\udd0d FEATURE IMPORTANCE ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Extract feature importance (if available)\n",
        "try:\n",
        "    # Get feature names after preprocessing\n",
        "    feature_names = []\n",
        "    \n",
        "    # Add numeric features\n",
        "    feature_names.extend(numeric_features)\n",
        "    \n",
        "    # Add categorical features (after one-hot encoding)\n",
        "    if categorical_features:\n",
        "        # Get feature names from the preprocessor\n",
        "        cat_transformer = final_model.named_steps['preprocessor'].named_transformers_['cat']\n",
        "        cat_feature_names = cat_transformer.get_feature_names_out(categorical_features)\n",
        "        feature_names.extend(cat_feature_names)\n",
        "    \n",
        "    # Get feature importance from the classifier\n",
        "    classifier = final_model.named_steps['classifier']\n",
        "    \n",
        "    if hasattr(classifier, 'feature_importances_'):\n",
        "        importance_scores = classifier.feature_importances_\n",
        "        \n",
        "        # Create feature importance DataFrame\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'Feature': feature_names,\n",
        "            'Importance': importance_scores\n",
        "        }).sort_values('Importance', ascending=False)\n",
        "        \n",
        "        print(f\"\ud83c\udfc6 Top 10 Most Important Features:\")\n",
        "        display(feature_importance.head(10))\n",
        "        \n",
        "        # Visualize feature importance\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        \n",
        "        # Plot top 15 features\n",
        "        top_features = feature_importance.head(15)\n",
        "        \n",
        "        plt.subplot(1, 2, 1)\n",
        "        sns.barplot(data=top_features, y='Feature', x='Importance')\n",
        "        plt.title('Top 15 Feature Importance', fontweight='bold')\n",
        "        plt.xlabel('Importance Score')\n",
        "        \n",
        "        # Cumulative importance\n",
        "        plt.subplot(1, 2, 2)\n",
        "        cumulative_importance = feature_importance['Importance'].cumsum()\n",
        "        plt.plot(range(1, len(cumulative_importance) + 1), cumulative_importance, 'b-', linewidth=2)\n",
        "        plt.axhline(y=0.8, color='r', linestyle='--', alpha=0.7, label='80% threshold')\n",
        "        plt.axhline(y=0.9, color='orange', linestyle='--', alpha=0.7, label='90% threshold')\n",
        "        plt.xlabel('Number of Features')\n",
        "        plt.ylabel('Cumulative Importance')\n",
        "        plt.title('Cumulative Feature Importance', fontweight='bold')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Insights from feature importance\n",
        "        print(f\"\\n\ud83d\udca1 Feature Importance Insights:\")\n",
        "        total_importance = feature_importance['Importance'].sum()\n",
        "        top_3_importance = feature_importance.head(3)['Importance'].sum()\n",
        "        \n",
        "        print(f\"  \u2022 Top 3 features explain {top_3_importance:.1%} of model decisions\")\n",
        "        \n",
        "        # Find how many features needed for 80% importance\n",
        "        features_for_80pct = (cumulative_importance >= 0.8).idxmax() + 1\n",
        "        print(f\"  \u2022 {features_for_80pct} features needed to explain 80% of model decisions\")\n",
        "        \n",
        "        # Check if certain feature types dominate\n",
        "        numeric_importance = feature_importance[feature_importance['Feature'].isin(numeric_features)]['Importance'].sum()\n",
        "        print(f\"  \u2022 Numeric features contribute {numeric_importance:.1%} of total importance\")\n",
        "        \n",
        "        # Save feature importance\n",
        "        feature_importance.to_csv('../results/feature_importance.csv', index=False)\n",
        "        print(f\"\\n\ud83d\udcbe Feature importance saved to: ../results/feature_importance.csv\")\n",
        "        \n",
        "    elif hasattr(classifier, 'coef_'):\n",
        "        # For linear models, use coefficient magnitudes\n",
        "        importance_scores = np.abs(classifier.coef_[0])\n",
        "        \n",
        "        feature_importance = pd.DataFrame({\n",
        "            'Feature': feature_names,\n",
        "            'Importance': importance_scores\n",
        "        }).sort_values('Importance', ascending=False)\n",
        "        \n",
        "        print(f\"\ud83c\udfc6 Top 10 Most Important Features (by coefficient magnitude):\")\n",
        "        display(feature_importance.head(10))\n",
        "        \n",
        "        # Simple visualization for coefficients\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        top_features = feature_importance.head(10)\n",
        "        sns.barplot(data=top_features, y='Feature', x='Importance')\n",
        "        plt.title('Top 10 Feature Importance (Coefficient Magnitude)', fontweight='bold')\n",
        "        plt.xlabel('Coefficient Magnitude')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "    else:\n",
        "        print(\"\u26a0\ufe0f  Feature importance not available for this model type\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error extracting feature importance: {str(e)}\")\n",
        "    print(\"\u26a0\ufe0f  Feature importance analysis skipped\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcca Model Evaluation {#evaluation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive model evaluation\n",
        "print(\"\ud83d\udcca COMPREHENSIVE MODEL EVALUATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Classification Report\n",
        "print(\"\ud83d\udccb CLASSIFICATION REPORT:\")\n",
        "print(\"-\" * 50)\n",
        "class_report = classification_report(y_test, final_predictions, \n",
        "                                   target_names=['Stayed', 'Left'],\n",
        "                                   output_dict=True)\n",
        "print(classification_report(y_test, final_predictions, target_names=['Stayed', 'Left']))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, final_predictions)\n",
        "print(f\"\\n\ud83c\udfaf CONFUSION MATRIX:\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Create detailed confusion matrix visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Confusion Matrix Heatmap\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0],\n",
        "           xticklabels=['Stayed', 'Left'], yticklabels=['Stayed', 'Left'])\n",
        "axes[0,0].set_title('Confusion Matrix', fontweight='bold')\n",
        "axes[0,0].set_ylabel('Actual')\n",
        "axes[0,0].set_xlabel('Predicted')\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, final_probabilities)\n",
        "roc_auc = roc_auc_score(y_test, final_probabilities)\n",
        "\n",
        "axes[0,1].plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
        "axes[0,1].plot([0, 1], [0, 1], 'k--', alpha=0.7, label='Random Classifier')\n",
        "axes[0,1].set_xlabel('False Positive Rate')\n",
        "axes[0,1].set_ylabel('True Positive Rate')\n",
        "axes[0,1].set_title('ROC Curve', fontweight='bold')\n",
        "axes[0,1].legend()\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Precision-Recall Curve\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "precision_curve, recall_curve, _ = precision_recall_curve(y_test, final_probabilities)\n",
        "avg_precision = average_precision_score(y_test, final_probabilities)\n",
        "\n",
        "axes[1,0].plot(recall_curve, precision_curve, linewidth=2, \n",
        "              label=f'PR Curve (AP = {avg_precision:.3f})')\n",
        "axes[1,0].set_xlabel('Recall')\n",
        "axes[1,0].set_ylabel('Precision')\n",
        "axes[1,0].set_title('Precision-Recall Curve', fontweight='bold')\n",
        "axes[1,0].legend()\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Prediction Probability Distribution\n",
        "axes[1,1].hist(final_probabilities[y_test == 0], bins=30, alpha=0.7, \n",
        "              label='Stayed (Actual)', color='blue', density=True)\n",
        "axes[1,1].hist(final_probabilities[y_test == 1], bins=30, alpha=0.7, \n",
        "              label='Left (Actual)', color='red', density=True)\n",
        "axes[1,1].axvline(x=0.5, color='black', linestyle='--', alpha=0.7, label='Threshold (0.5)')\n",
        "axes[1,1].set_xlabel('Predicted Probability')\n",
        "axes[1,1].set_ylabel('Density')\n",
        "axes[1,1].set_title('Prediction Probability Distribution', fontweight='bold')\n",
        "axes[1,1].legend()\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Model Performance Summary\n",
        "print(f\"\\n\ud83c\udfc6 FINAL MODEL PERFORMANCE SUMMARY:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Model: {best_model_name}\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, final_predictions):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, final_predictions):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, final_predictions):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, final_predictions):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test, final_probabilities):.4f}\")\n",
        "\n",
        "# Business Impact Analysis\n",
        "print(f\"\\n\ud83d\udcbc BUSINESS IMPACT ANALYSIS:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "total_predictions = tn + fp + fn + tp\n",
        "\n",
        "print(f\"True Negatives (Correctly predicted to stay): {tn:,} ({tn/total_predictions:.1%})\")\n",
        "print(f\"True Positives (Correctly predicted to leave): {tp:,} ({tp/total_predictions:.1%})\")\n",
        "print(f\"False Negatives (Missed departures): {fn:,} ({fn/total_predictions:.1%})\")\n",
        "print(f\"False Positives (False alarms): {fp:,} ({fp/total_predictions:.1%})\")\n",
        "\n",
        "# Cost-benefit analysis (hypothetical)\n",
        "print(f\"\\n\ud83d\udcb0 HYPOTHETICAL COST-BENEFIT ANALYSIS:\")\n",
        "print(\"-\" * 50)\n",
        "cost_per_departure = 50000  # Hypothetical cost of employee turnover\n",
        "cost_per_intervention = 2000  # Hypothetical cost of retention intervention\n",
        "\n",
        "# Calculate potential savings\n",
        "employees_correctly_identified = tp\n",
        "false_alarms = fp\n",
        "missed_departures = fn\n",
        "\n",
        "savings_from_retention = employees_correctly_identified * cost_per_departure\n",
        "cost_of_interventions = (employees_correctly_identified + false_alarms) * cost_per_intervention\n",
        "cost_of_missed_departures = missed_departures * cost_per_departure\n",
        "\n",
        "net_benefit = savings_from_retention - cost_of_interventions - cost_of_missed_departures\n",
        "\n",
        "print(f\"Potential savings from identified at-risk employees: ${savings_from_retention:,.0f}\")\n",
        "print(f\"Cost of retention interventions: ${cost_of_interventions:,.0f}\")\n",
        "print(f\"Cost of missed departures: ${cost_of_missed_departures:,.0f}\")\n",
        "print(f\"Net benefit: ${net_benefit:,.0f}\")\n",
        "\n",
        "if net_benefit > 0:\n",
        "    print(f\"\u2705 Model provides positive ROI!\")\n",
        "else:\n",
        "    print(f\"\u26a0\ufe0f  Model may need improvement for positive ROI\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcbe Model Persistence {#persistence}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the final model and evaluation results\n",
        "print(\"\ud83d\udcbe SAVING MODEL AND RESULTS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "try:\n",
        "    # Create results directory if it doesn't exist\n",
        "    import os\n",
        "    os.makedirs('../results', exist_ok=True)\n",
        "    \n",
        "    # Save the trained model\n",
        "    model_path = '../results/best_employee_retention_model.joblib'\n",
        "    joblib.dump(final_model, model_path)\n",
        "    print(f\"\u2705 Model saved to: {model_path}\")\n",
        "    \n",
        "    # Save model metadata\n",
        "    model_metadata = {\n",
        "        'model_name': best_model_name,\n",
        "        'model_type': str(type(final_model.named_steps['classifier']).__name__),\n",
        "        'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'training_samples': X_train.shape[0],\n",
        "        'test_samples': X_test.shape[0],\n",
        "        'features_used': len(feature_names) if 'feature_names' in locals() else X.shape[1],\n",
        "        'performance_metrics': {\n",
        "            'accuracy': float(accuracy_score(y_test, final_predictions)),\n",
        "            'precision': float(precision_score(y_test, final_predictions)),\n",
        "            'recall': float(recall_score(y_test, final_predictions)),\n",
        "            'f1_score': float(f1_score(y_test, final_predictions)),\n",
        "            'roc_auc': float(roc_auc_score(y_test, final_probabilities))\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Save metadata as JSON\n",
        "    import json\n",
        "    metadata_path = '../results/model_metadata.json'\n",
        "    with open(metadata_path, 'w') as f:\n",
        "        json.dump(model_metadata, f, indent=2)\n",
        "    print(f\"\ud83d\udccb Model metadata saved to: {metadata_path}\")\n",
        "    \n",
        "    # Save test predictions\n",
        "    test_results = pd.DataFrame({\n",
        "        'actual': y_test.values,\n",
        "        'predicted': final_predictions,\n",
        "        'probability': final_probabilities\n",
        "    })\n",
        "    \n",
        "    results_path = '../results/test_predictions.csv'\n",
        "    test_results.to_csv(results_path, index=False)\n",
        "    print(f\"\ud83c\udfaf Test predictions saved to: {results_path}\")\n",
        "    \n",
        "    # Save classification report\n",
        "    report_df = pd.DataFrame(class_report).transpose()\n",
        "    report_path = '../results/classification_report.csv'\n",
        "    report_df.to_csv(report_path)\n",
        "    print(f\"\ud83d\udcca Classification report saved to: {report_path}\")\n",
        "    \n",
        "    print(f\"\\n\ud83c\udf89 All model artifacts saved successfully!\")\n",
        "    print(f\"\ud83d\udcc1 Results directory: ../results/\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error saving model artifacts: {str(e)}\")\n",
        "\n",
        "print(f\"\\n\" + \"=\"*50)\n",
        "print(f\"\u2705 MODEL DEVELOPMENT COMPLETE!\")\n",
        "print(f\"\ud83c\udfc6 Best Model: {best_model_name}\")\n",
        "print(f\"\ud83d\udcca F1-Score: {f1_score(y_test, final_predictions):.4f}\")\n",
        "print(f\"\ud83c\udfaf Next step: 04_results.ipynb for business insights\")\n",
        "print(f\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udccb Modeling Summary\n",
        "\n",
        "### \u2705 Completed Tasks:\n",
        "1. **Data Preparation** - Preprocessed features and split data appropriately\n",
        "2. **Baseline Models** - Trained and compared 5 different algorithms\n",
        "3. **Model Comparison** - Evaluated performance using multiple metrics\n",
        "4. **Hyperparameter Tuning** - Optimized the best performing model\n",
        "5. **Feature Importance** - Identified key drivers of employee retention\n",
        "6. **Model Evaluation** - Comprehensive performance analysis\n",
        "7. **Model Persistence** - Saved trained model and results\n",
        "\n",
        "### \ud83c\udfc6 Key Achievements:\n",
        "- Built robust machine learning pipeline for employee retention prediction\n",
        "- Achieved high-quality predictions with proper validation\n",
        "- Identified most important features influencing employee decisions\n",
        "- Provided business impact analysis and ROI calculations\n",
        "\n",
        "### \ud83d\udcca Model Performance:\n",
        "The final model demonstrates strong predictive capability and can effectively identify employees at risk of leaving, enabling proactive retention strategies.\n",
        "\n",
        "### \ud83d\ude80 Next Steps:\n",
        "The trained model is ready for business interpretation and actionable insights in the results analysis phase.\n",
        "\n",
        "---\n",
        "\n",
        "**\ud83e\udd16 Model Development Complete!**\n",
        "\n",
        "*Next notebook: `04_results.ipynb`*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
